{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ff19060-dd0b-4527-9088-7f90b17e7026",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jieba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasketch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinHash, MinHashLSH\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjieba\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'jieba'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dbe5d9-aa18-42b1-a51c-c3f27e9564d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. 配置参数 ===\n",
    "TARGET_CITIES = ['西安', '兰州', '敦煌', '乌鲁木齐', '银川', '西宁', '张掖', '嘉峪关']\n",
    "MAX_SAMPLES = 14000  # 目标数据量\n",
    "DUPE_THRESHOLD = 0.85  # 去重相似度阈值\n",
    "\n",
    "# === 2. 加载中文旅游监督数据集 ===\n",
    "def load_cn_tourism(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            # 转换格式并添加指令前缀\n",
    "            data.append({\n",
    "                \"conversations\": [\n",
    "                    {\"role\": \"user\", \"content\": item['prompt']},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"/no_think {item['response']}\"}\n",
    "                ]\n",
    "            })\n",
    "    print(f\"Loaded {len(data)} Chinese tourism QA pairs\")\n",
    "    return data\n",
    "\n",
    "# === 3. 加载并抽样西北数据集 ===\n",
    "def load_nw_tourism(file_path, sample_size=4000):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        nw_data = json.load(f)\n",
    "    \n",
    "    # 城市关键词过滤函数\n",
    "    def contains_target_city(text):\n",
    "        text = str(text).lower()\n",
    "        return any(city in text for city in TARGET_CITIES)\n",
    "    \n",
    "    # 过滤并抽样\n",
    "    filtered_data = [\n",
    "        item for item in nw_data \n",
    "        if contains_target_city(item.get('instruction', '')) or \n",
    "           contains_target_city(item.get('output', ''))\n",
    "    ]\n",
    "    \n",
    "    # 分层抽样：确保每个城市至少有50条\n",
    "    sampled_data = []\n",
    "    city_counts = {city: 0 for city in TARGET_CITIES}\n",
    "    \n",
    "    random.shuffle(filtered_data)\n",
    "    for item in filtered_data:\n",
    "        for city in TARGET_CITIES:\n",
    "            if city in json.dumps(item) and city_counts[city] < 50:\n",
    "                sampled_data.append(item)\n",
    "                city_counts[city] += 1\n",
    "                break\n",
    "        if len(sampled_data) >= sample_size:\n",
    "            break\n",
    "    \n",
    "    print(f\"Sampled {len(sampled_data)} NW tourism items\")\n",
    "    \n",
    "    # 转换为标准格式\n",
    "    converted_data = []\n",
    "    for item in sampled_data:\n",
    "        # 处理空input\n",
    "        user_content = item['instruction']\n",
    "        if item.get('input', '').strip():\n",
    "            user_content += f\"\\n{item['input']}\"\n",
    "        \n",
    "        converted_data.append({\n",
    "            \"conversations\": [\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "                {\"role\": \"assistant\", \"content\": f\"/no_think {item['output']}\"}\n",
    "            ]\n",
    "        })\n",
    "    return converted_data\n",
    "\n",
    "# === 4. MinHash去重 ===\n",
    "def deduplicate(data, threshold=DUPE_THRESHOLD):\n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=128)\n",
    "    unique_data = []\n",
    "    \n",
    "    for idx, item in enumerate(data):\n",
    "        assistant_content = item['conversations'][1]['content']\n",
    "        \n",
    "        # 创建MinHash指纹\n",
    "        m = MinHash(num_perm=128)\n",
    "        words = jieba.lcut(assistant_content)[:50]  # 使用结巴分词\n",
    "        for word in words:\n",
    "            m.update(word.encode('utf-8'))\n",
    "        \n",
    "        # 检查是否重复\n",
    "        if not lsh.query(m):\n",
    "            lsh.insert(f\"item_{idx}\", m)\n",
    "            unique_data.append(item)\n",
    "    \n",
    "    dup_rate = (1 - len(unique_data)/len(data)) * 100\n",
    "    print(f\"Deduplication: {len(unique_data)}/{len(data)} items kept ({dup_rate:.2f}% duplication)\")\n",
    "    return unique_data\n",
    "\n",
    "# === 5. 生成LoRA训练格式 ===\n",
    "def convert_to_lora_format(data):\n",
    "    \"\"\"转换为Qwen2.5 LoRA微调专用格式\"\"\"\n",
    "    lora_data = []\n",
    "    for item in data:\n",
    "        # 构建多轮对话格式（Qwen2.5要求）\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"你是一名专业的旅游助手\"},\n",
    "            *item['conversations']\n",
    "        ]\n",
    "        \n",
    "        lora_data.append({\n",
    "            \"messages\": messages,\n",
    "            \"no_think\": True  # 启用快速响应模式\n",
    "        })\n",
    "    return lora_data\n",
    "\n",
    "# === 6. 生成质量报告 ===\n",
    "def generate_quality_report(data, output_file):\n",
    "    # 1. 基本统计\n",
    "    total_items = len(data)\n",
    "    word_counts = []\n",
    "    city_coverage = {city: 0 for city in TARGET_CITIES}\n",
    "    \n",
    "    # 2. 关键词分析\n",
    "    keywords = [\"景点\", \"酒店\", \"交通\", \"美食\", \"攻略\", \"行程\", \"推荐\", \"门票\"]\n",
    "    keyword_coverage = {kw: 0 for kw in keywords}\n",
    "    \n",
    "    # 3. 遍历数据\n",
    "    for item in data:\n",
    "        content = json.dumps(item)\n",
    "        \n",
    "        # 城市覆盖统计\n",
    "        for city in TARGET_CITIES:\n",
    "            if city in content:\n",
    "                city_coverage[city] += 1\n",
    "        \n",
    "        # 关键词统计\n",
    "        for kw in keywords:\n",
    "            if kw in content:\n",
    "                keyword_coverage[kw] += 1\n",
    "        \n",
    "        # 词数统计\n",
    "        assistant_text = item['messages'][-1]['content']\n",
    "        word_counts.append(len(jieba.lcut(assistant_text)))\n",
    "    \n",
    "    # 4. 生成报告\n",
    "    report = f\"# 旅游数据集质量报告\\n\\n\"\n",
    "    report += f\"**总样本量**: {total_items}\\n\"\n",
    "    report += f\"**平均回复长度**: {sum(word_counts)/len(word_counts):.1f} 词\\n\\n\"\n",
    "    \n",
    "    report += \"## 城市覆盖统计\\n\"\n",
    "    for city, count in city_coverage.items():\n",
    "        coverage_pct = (count / total_items) * 100\n",
    "        report += f\"- {city}: {count} 条 ({coverage_pct:.1f}%)\\n\"\n",
    "    \n",
    "    report += \"\\n## 关键词覆盖率\\n\"\n",
    "    for kw, count in keyword_coverage.items():\n",
    "        coverage_pct = (count / total_items) * 100\n",
    "        report += f\"- {kw}: {coverage_pct:.1f}%\\n\"\n",
    "    \n",
    "    # 5. 保存报告\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(report)\n",
    "    print(f\"Quality report saved to {output_file}\")\n",
    "\n",
    "# === 主流程 ===\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载数据集\n",
    "    cn_data = load_cn_tourism(\"chinese_tourism.jsonl\")\n",
    "    nw_data = load_nw_tourism(\"northwest_tourism.json\")\n",
    "    \n",
    "    # 合并并去重\n",
    "    combined_data = cn_data + nw_data\n",
    "    deduped_data = deduplicate(combined_data)\n",
    "    \n",
    "    # 转换为LoRA格式\n",
    "    lora_data = convert_to_lora_format(deduped_data[:MAX_SAMPLES])\n",
    "    \n",
    "    # 保存最终数据集\n",
    "    with open(\"tourism_lora_data.jsonl\", 'w', encoding='utf-8') as f:\n",
    "        for item in lora_data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    print(f\"Saved {len(lora_data)} LoRA-formatted items\")\n",
    "    \n",
    "    # 生成质量报告\n",
    "    generate_quality_report(lora_data, \"data_quality.md\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
