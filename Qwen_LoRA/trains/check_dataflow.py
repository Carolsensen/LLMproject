# check_dataflow.py
# check_dataflow_fixed.py
import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from data_collator import FixedDataCollator  # éœ€å•ç‹¬ä¿å­˜

# é…ç½®å‚æ•°
DATA_PATH = "/mnt/workspace/qwen_model/Qwen/Qwen2.5-7B-Instruct/tourism_data.jsonl"
MODEL_PATH = "/mnt/workspace/qwen_model/Qwen/Qwen2.5-7B-Instruct"

def format_dataset(example):
    """ä¿®æ­£åçš„æ•°æ®æ ¼å¼åŒ–å‡½æ•°"""
    messages = example["messages"]
    formatted_text = ""
    for msg in messages:
        if isinstance(msg, dict):
            content = msg["content"]
            if msg["role"] == "assistant" and content.startswith("/no_think"):
                content = content[9:].strip()
            formatted_text += f"{msg['role'].capitalize()}: {content}\n"
    formatted_text += tokenizer.eos_token
    return {"text": formatted_text}

# åˆå§‹åŒ–
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

# 1. æ•°æ®åŠ è½½æµ‹è¯•
try:
    dataset = load_dataset("json", data_files=DATA_PATH, split="train")
    dataset = dataset.map(format_dataset, remove_columns=["messages"])
    print("âœ… æ•°æ®åŠ è½½æˆåŠŸ")
    print("æ ·æœ¬å­—æ®µ:", dataset.column_names)
    print("é¦–æ ·æœ¬textå†…å®¹:", dataset[0]["text"][:100] + "...")
except Exception as e:
    print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
    exit()

# 2. æ•°æ®æ”¶é›†å™¨æµ‹è¯•
try:
    collator = FixedDataCollator(tokenizer=tokenizer, mlm=False)  # å…³é”®ä¿®æ­£ï¼šæ·»åŠ mlm=False
    batch = collator([dataset[0], dataset[1]])
    
    print("\nğŸ” æ‰¹æ¬¡æ£€æŸ¥:")
    print("input_idså½¢çŠ¶:", batch["input_ids"].shape)
    print("input_idsç¤ºä¾‹:", batch["input_ids"][0][:10])
    print("labelsä¸input_idsä¸€è‡´?", torch.all(batch["input_ids"] == batch["labels"]))
    
    if batch["input_ids"].sum().item() == 0:
        print("âŒ è­¦å‘Šï¼šæ‰¹æ¬¡è¾“å…¥å…¨ä¸ºé›¶ï¼")
    else:
        print("âœ… æ‰¹æ¬¡æ•°æ®æœ‰æ•ˆ")
        
    # GPUæµ‹è¯•
    device_batch = {k: v.to("cuda") for k, v in batch.items()}
    print("âœ… GPUæ•°æ®ä¼ è¾“æˆåŠŸ")
except Exception as e:
    print(f"âŒ æ•°æ®æ”¶é›†å™¨æµ‹è¯•å¤±è´¥: {str(e)}")
    exit()

# 3. æ¨¡å‹å‰å‘ä¼ æ’­æµ‹è¯•
try:
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        device_map="auto",
        torch_dtype=torch.float16
    )
    outputs = model(**device_batch)  # ä½¿ç”¨GPUæ‰¹æ¬¡
    loss = outputs.loss
    print(f"\nğŸ¯ å‰å‘ä¼ æ’­æµ‹è¯•: loss={loss.item():.4f}")
    if loss.item() == 0:
        print("âŒ è­¦å‘Šï¼šLossä¸ºé›¶ï¼Œè¯·æ£€æŸ¥æ•°æ®ï¼")
except Exception as e:
    print(f"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {str(e)}")