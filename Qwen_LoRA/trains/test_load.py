# test_load.py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# é…ç½®å‚æ•°
MODEL_PATH = "/mnt/workspace/qwen_model/Qwen/Qwen2.5-7B-Instruct"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

try:
    # 1. åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_PATH,
        trust_remote_code=True
    )
    print("âœ… TokenizeråŠ è½½æˆåŠŸ")

    # 2. åŠ è½½æ¨¡å‹ï¼ˆ4-bité‡åŒ–èŠ‚çœæ˜¾å­˜ï¼‰
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        device_map="auto",
        torch_dtype=torch.float16,
        load_in_4bit=True,  # 24GBæ˜¾å­˜å»ºè®®å¯ç”¨
        trust_remote_code=True
    )
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ | æ˜¾å­˜å ç”¨: {torch.cuda.memory_allocated()/1024**3:.1f}GB")

    # 3. æµ‹è¯•æ¨ç†
    input_text = "å¤©æ°´éº¦ç§¯å±±çŸ³çªŸæœ‰ä»€ä¹ˆç‰¹è‰²ï¼Ÿ"
    inputs = tokenizer(input_text, return_tensors="pt").to(DEVICE)
    
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=100)
    
    print("\nğŸ” æµ‹è¯•è¾“å‡ºï¼š")
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))

except Exception as e:
    print(f"âŒ åŠ è½½å¤±è´¥: {str(e)}")
    print("\næ’æŸ¥å»ºè®®ï¼š")
    print("1. æ£€æŸ¥æ–‡ä»¶æƒé™ï¼šls -l /mnt/workspace/qwen_model/")
    print("2. éªŒè¯CUDAæ˜¯å¦å¯ç”¨ï¼špython -c 'import torch; print(torch.cuda.is_available())'")
    print("3. å°è¯•ç®€åŒ–åŠ è½½ï¼šå…ˆä»…åŠ è½½tokenizeræµ‹è¯•")