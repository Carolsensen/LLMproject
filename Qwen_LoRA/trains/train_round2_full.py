# train_round2_full.py
import torch
import pandas as pd
import os
from datetime import datetime
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    BitsAndBytesConfig,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig
from trl import SFTTrainer
from datasets import load_dataset

# ================= é…ç½®åŒº =================
MODEL_PATH = "/mnt/workspace/qwen_model/Qwen/Qwen2.5-7B-Instruct"
DATA_PATH = "/mnt/workspace/qwen_model/Qwen/Qwen2.5-7B-Instruct/tourism_data.jsonl"
OUTPUT_DIR = "/mnt/workspace/qwen_model/round2_output"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ================= æ•°æ®æ ¼å¼åŒ– =================
def format_dataset(example):
    """ä¸¥æ ¼ç¡®ä¿è¾“å‡ºçº¯æ–‡æœ¬"""
    messages = example["messages"]
    text = ""
    for msg in messages:
        if isinstance(msg, dict):
            content = str(msg.get("content", "")).strip()  # å¼ºåˆ¶è½¬ä¸ºå­—ç¬¦ä¸²
            if msg.get("role") == "assistant" and content.startswith("/no_think"):
                content = content[9:]
            text += f"{msg.get('role', 'user').capitalize()}: {content}\n"
    return {"text": text + tokenizer.eos_token}  # ç¡®ä¿EOSæ ‡è®°

# ================= å®‰å…¨çš„æ•°æ®æ”¶é›†å™¨ =================
class SafeDataCollator(DataCollatorForLanguageModeling):
    def __call__(self, examples):
        # è¿‡æ»¤æ— æ•ˆæ ·æœ¬
        texts = []
        for ex in examples:
            if isinstance(ex, dict) and "text" in ex and isinstance(ex["text"], str):
                texts.append(ex["text"])
            else:
                print(f"âš ï¸ å¿½ç•¥æ— æ•ˆæ ·æœ¬: {type(ex)}")
        
        if not texts:
            raise ValueError("æ‰€æœ‰æ ·æœ¬å‡æ— æ•ˆï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼ï¼")
            
        # å¼ºåˆ¶å¯ç”¨paddingå’Œtruncation
        batch = self.tokenizer(
            texts,
            padding="longest",
            truncation=True,
            max_length=1024,
            return_tensors="pt"
        )
        batch["labels"] = batch["input_ids"].clone()
        return batch

# ================= è‡ªå®šä¹‰Trainerï¼ˆæ·»åŠ lossè®°å½•ï¼‰ =================
class LossTrackingTrainer(SFTTrainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.loss_history = []  # å­˜å‚¨lossè®°å½•ï¼š[(step, loss, timestamp), ...]
        self.start_time = datetime.now()  # è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´

    def training_step(self, model, inputs):
        # è°ƒç”¨çˆ¶ç±»çš„è®­ç»ƒæ­¥éª¤ï¼Œè·å–loss
        loss = super().training_step(model, inputs)
        
        # è®°å½•å½“å‰æ­¥æ•°ã€losså’Œæ—¶é—´
        current_step = self.state.global_step  # è·å–å…¨å±€æ­¥æ•°
        elapsed_time = (datetime.now() - self.start_time).total_seconds()
        self.loss_history.append({
            "step": current_step,
            "loss": loss.item(),
            "elapsed_time": elapsed_time
        })
        
        # æ¯10æ­¥ï¼ˆä¸logging_stepsä¸€è‡´ï¼‰ä¸´æ—¶ä¿å­˜ä¸€æ¬¡
        if current_step % self.args.logging_steps == 0 and current_step > 0:
            self.save_loss_history()
        
        # æ¯100æ­¥ï¼ˆä¸save_stepsä¸€è‡´ï¼‰å¼ºåˆ¶ä¿å­˜ä¸€æ¬¡
        if current_step % self.args.save_steps == 0 and current_step > 0:
            self.save_loss_history()
        
        return loss

    def save_loss_history(self):
        """å°†losså†å²ä¿å­˜åˆ°CSVæ–‡ä»¶"""
        loss_file = os.path.join(self.args.output_dir, "loss_history.csv")
        df = pd.DataFrame(self.loss_history)
        df.to_csv(loss_file, index=False)
        print(f"ğŸ’¾ å·²ä¿å­˜losså†å²åˆ° {loss_file}ï¼ˆæœ€æ–°æ­¥æ•°: {self.state.global_step}ï¼‰")

    def _save_checkpoint(self, *args, **kwargs):
        """ä¿å­˜checkpointæ—¶é¢å¤–ç¡®ä¿losså†å²è¢«ä¿å­˜"""
        super()._save_checkpoint(*args, **kwargs)
        self.save_loss_history()

# ================= åˆå§‹åŒ– =================
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

# åŠ è½½å¹¶éªŒè¯æ•°æ®
dataset = load_dataset("json", data_files=DATA_PATH, split="train")
dataset = dataset.map(format_dataset, remove_columns=["messages"])
print("âœ… æ•°æ®æ ·æœ¬ç¤ºä¾‹:", dataset[0]["text"][:100] + "...")

# ================= è®­ç»ƒé…ç½® =================
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=5e-5,
    num_train_epochs=1,
    evaluation_strategy="no",
    save_strategy="steps",
    save_steps=100,  # æ¯100æ­¥ä¿å­˜checkpoint
    logging_steps=10,  # æ¯10æ­¥è®°å½•æ—¥å¿—
    fp16=True,
    remove_unused_columns=False,
    optim="adamw_torch_fused",
    max_grad_norm=0.3,
    warmup_ratio=0.1,
    lr_scheduler_type="cosine",
    report_to="tensorboard",
    gradient_checkpointing=True,
    dataloader_num_workers=2,
)

# ================= å¯åŠ¨è®­ç»ƒ =================
# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    device_map="auto",
    quantization_config=BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True
    ),
    trust_remote_code=True
)

# ä½¿ç”¨è‡ªå®šä¹‰Trainerï¼ˆå¸¦lossè®°å½•åŠŸèƒ½ï¼‰
trainer = LossTrackingTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    peft_config=LoraConfig(
        r=8,
        lora_alpha=32,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        lora_dropout=0.05,
        task_type="CAUSAL_LM"
    ),
    max_seq_length=1024,
    dataset_text_field="text",
    tokenizer=tokenizer,
    data_collator=SafeDataCollator(tokenizer=tokenizer, mlm=False)
)

print("å¼€å§‹è®­ç»ƒ...")
trainer.train()
trainer.save_model(f"{OUTPUT_DIR}/final")
# è®­ç»ƒç»“æŸåæœ€ç»ˆä¿å­˜ä¸€æ¬¡losså†å²
trainer.save_loss_history()